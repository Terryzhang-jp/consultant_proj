#!/usr/bin/env python3
"""
è¯Šæ–­Hypothesis 1ç»“æœå·®å¼‚çš„è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from hypothesis_testing.hypothesis_1.hypothesis_1 import Hypothesis1ValidatorCorrect
import pandas as pd

def analyze_data_characteristics():
    """åˆ†ææ•°æ®ç‰¹å¾"""
    print("ğŸ” åˆ†ææ•°æ®ç‰¹å¾...")
    
    validator = Hypothesis1ValidatorCorrect()
    
    # åŠ è½½åŸå§‹æ•°æ®
    final_dataset = validator.data_loader.create_final_dataset(
        include_lp_history=True,
        include_reward=True,
        include_discipline=True
    )
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»è®°å½•æ•°: {len(final_dataset):,}")
    print(f"   æ€»LPæ•°: {final_dataset['LP'].nunique():,}")
    print(f"   å¹´ä»½èŒƒå›´: {final_dataset['S_YR'].min()}-{final_dataset['S_YR'].max()}")
    print(f"   RANKåˆ†å¸ƒ: {final_dataset['RANK_x'].value_counts().to_dict()}")
    
    # åˆ†æRANK=10çš„æ•°æ®
    rank_10_data = final_dataset[final_dataset['RANK_x'] == 10]
    print(f"\nğŸ“ˆ RANK=10æ•°æ®ç»Ÿè®¡:")
    print(f"   RANK=10è®°å½•æ•°: {len(rank_10_data):,}")
    print(f"   RANK=10 LPæ•°: {rank_10_data['LP'].nunique():,}")
    print(f"   å¹´ä»½èŒƒå›´: {rank_10_data['S_YR'].min()}-{rank_10_data['S_YR'].max()}")
    
    # åˆ†ææ‡²æˆ’å‡¦åˆ†æ•°æ®
    discipline_data = final_dataset[final_dataset['SHOBUN_FLAG'] == 1]
    print(f"\nâš–ï¸  æ‡²æˆ’å‡¦åˆ†æ•°æ®ç»Ÿè®¡:")
    print(f"   æ‡²æˆ’å‡¦åˆ†è®°å½•æ•°: {len(discipline_data):,}")
    print(f"   æ‡²æˆ’å‡¦åˆ†LPæ•°: {discipline_data['LP'].nunique():,}")
    print(f"   å¹´ä»½åˆ†å¸ƒ: {discipline_data['S_YR'].value_counts().sort_index().head(10).to_dict()}")
    
    return final_dataset

def analyze_hypothesis_1_processing(final_dataset):
    """åˆ†æHypothesis 1çš„å¤„ç†è¿‡ç¨‹"""
    print("\nğŸ¯ åˆ†æHypothesis 1å¤„ç†è¿‡ç¨‹...")
    
    validator = Hypothesis1ValidatorCorrect()
    
    # è¿è¡Œåˆ†æ
    analysis_results = validator._analyze_lp_sashihiki_with_shobun_optimized(final_dataset)
    
    print(f"ğŸ“‹ å¤„ç†åæ•°æ®ç»Ÿè®¡:")
    print(f"   åˆ†æè®°å½•æ•°: {len(analysis_results):,}")
    print(f"   å”¯ä¸€LPæ•°: {analysis_results['LP'].nunique():,}")
    print(f"   å”¯ä¸€FISCAL_HALFæ•°: {analysis_results['FISCAL_HALF'].nunique():,}")
    
    # åˆ†æå˜æ•°1å’Œå˜æ•°2
    var1_count = (analysis_results['Sustained_Low'] == 1).sum()
    var2_count = (analysis_results['SHOBUN_in_Next_Half'] == 1).sum()
    both_count = ((analysis_results['Sustained_Low'] == 1) & 
                  (analysis_results['SHOBUN_in_Next_Half'] == 1)).sum()
    
    print(f"\nğŸ“Š å˜æ•°ç»Ÿè®¡:")
    print(f"   å˜æ•°1 (Sustained_Low=1): {var1_count:,}")
    print(f"   å˜æ•°2 (SHOBUN_in_Next_Half=1): {var2_count:,}")
    print(f"   ä¸¤è€…éƒ½æ»¡è¶³: {both_count:,}")
    
    # åˆ†æFISCAL_HALFåˆ†å¸ƒ
    fiscal_half_dist = analysis_results['FISCAL_HALF'].value_counts().sort_index()
    print(f"\nğŸ“… FISCAL_HALFåˆ†å¸ƒ (å‰10ä¸ª):")
    for fiscal_half, count in fiscal_half_dist.head(10).items():
        print(f"   {fiscal_half}: {count:,}")
    
    # åˆ†æå¹´ä»½åˆ†å¸ƒ
    year_dist = analysis_results.groupby('FISCAL_HALF').size().reset_index(name='count')
    year_dist['year'] = year_dist['FISCAL_HALF'].str.split('_').str[0].astype(int)
    yearly_summary = year_dist.groupby('year')['count'].sum().sort_index()
    
    print(f"\nğŸ“ˆ å¹´åº¦æ•°æ®åˆ†å¸ƒ (å‰10å¹´):")
    for year, count in yearly_summary.head(10).items():
        print(f"   {year}: {count:,}")
    
    return analysis_results

def compare_with_expected_results():
    """ä¸é¢„æœŸç»“æœè¿›è¡Œå¯¹æ¯”"""
    print("\nğŸ” ä¸å›¾ç‰‡ç»“æœå¯¹æ¯”åˆ†æ...")
    
    # å›¾ç‰‡ä¸­çš„é¢„æœŸç»“æœ
    expected_results = {
        'total_cases': 110391,
        'discipline_cases': 1448,
        'f1_score': 0.017,
        'total_lps': 10346  # ä»æè¿°æ¨æ–­
    }
    
    # å½“å‰ç»“æœ
    validator = Hypothesis1ValidatorCorrect()
    results = validator.run_hypothesis_1_with_data_loader()
    
    current_results = {
        'total_cases': results['data_shape'][0],
        'discipline_cases': results['sustained_low_analysis']['future_discipline_count'],
        'f1_score': results['statistical_test']['f1_score'],
        'total_lps': results['sustained_low_analysis']['total_lps']
    }
    
    print(f"ğŸ“Š ç»“æœå¯¹æ¯”:")
    print(f"   æŒ‡æ ‡                  | å›¾ç‰‡ç»“æœ    | å½“å‰ç»“æœ    | å·®å¼‚")
    print(f"   --------------------|-----------|-----------|----------")
    print(f"   æ€»æ¡ˆä¾‹æ•°             | {expected_results['total_cases']:,}      | {current_results['total_cases']:,}     | {current_results['total_cases'] - expected_results['total_cases']:+,}")
    print(f"   æ‡²æˆ’å‡¦åˆ†æ¡ˆä¾‹æ•°        | {expected_results['discipline_cases']:,}       | {current_results['discipline_cases']:,}      | {current_results['discipline_cases'] - expected_results['discipline_cases']:+,}")
    print(f"   F1åˆ†æ•°              | {expected_results['f1_score']:.3f}     | {current_results['f1_score']:.3f}     | {current_results['f1_score'] - expected_results['f1_score']:+.3f}")
    print(f"   æ€»LPæ•°              | {expected_results['total_lps']:,}      | {current_results['total_lps']:,}     | {current_results['total_lps'] - expected_results['total_lps']:+,}")
    
    # åˆ†æå¯èƒ½çš„åŸå› 
    print(f"\nğŸ¤” å¯èƒ½çš„å·®å¼‚åŸå› :")
    
    if current_results['total_lps'] > expected_results['total_lps']:
        print(f"   1. å½“å‰æ•°æ®åŒ…å«æ›´å¤šLP ({current_results['total_lps']:,} vs {expected_results['total_lps']:,})")
    
    if current_results['total_cases'] > expected_results['total_cases']:
        print(f"   2. å½“å‰æ•°æ®è¦†ç›–æ›´é•¿æ—¶é—´èŒƒå›´æˆ–æ›´å¤šè®°å½•")
    
    if current_results['discipline_cases'] > expected_results['discipline_cases']:
        print(f"   3. æ‡²æˆ’å‡¦åˆ†æ•°æ®åŒ¹é…æ–¹å¼å¯èƒ½ä¸åŒ")
    
    print(f"   4. æ•°æ®è¿‡æ»¤æ¡ä»¶å¯èƒ½ä¸åŒï¼ˆå¹´ä»½èŒƒå›´ã€RANKè¿‡æ»¤ç­‰ï¼‰")
    print(f"   5. çœŸå®æ•°æ® vs æµ‹è¯•æ•°æ®çš„å·®å¼‚")

def suggest_adjustments():
    """å»ºè®®è°ƒæ•´æ–¹æ¡ˆ"""
    print(f"\nğŸ’¡ å»ºè®®è°ƒæ•´æ–¹æ¡ˆ:")
    print(f"   1. æ£€æŸ¥å¹´ä»½è¿‡æ»¤èŒƒå›´ - å¯èƒ½éœ€è¦é™åˆ¶åˆ°ç‰¹å®šå¹´ä»½")
    print(f"   2. æ£€æŸ¥LPè¿‡æ»¤æ¡ä»¶ - å¯èƒ½éœ€è¦é¢å¤–çš„è¿‡æ»¤æ¡ä»¶")
    print(f"   3. æ£€æŸ¥æ‡²æˆ’å‡¦åˆ†æ•°æ®åŒ¹é…é€»è¾‘")
    print(f"   4. éªŒè¯FISCAL_HALFè®¡ç®—é€»è¾‘")
    print(f"   5. ç¡®è®¤rolling windowå‚æ•°ï¼ˆå½“å‰æ˜¯6ä¸ªåŠå¹´åº¦ï¼‰")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯Šæ–­Hypothesis 1ç»“æœå·®å¼‚...")
    print("=" * 80)
    
    try:
        # 1. åˆ†ææ•°æ®ç‰¹å¾
        final_dataset = analyze_data_characteristics()
        
        # 2. åˆ†æå¤„ç†è¿‡ç¨‹
        analysis_results = analyze_hypothesis_1_processing(final_dataset)
        
        # 3. ä¸é¢„æœŸç»“æœå¯¹æ¯”
        compare_with_expected_results()
        
        # 4. å»ºè®®è°ƒæ•´æ–¹æ¡ˆ
        suggest_adjustments()
        
        print("\n" + "=" * 80)
        print("ğŸ¯ è¯Šæ–­å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
