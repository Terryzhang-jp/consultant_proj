#!/usr/bin/env python3
"""
ä¿®å¤Hypothesis 1ä»¥åŒ¹é…mid.pyçš„ç¡®åˆ‡è®¾ç½®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from hypothesis_testing.hypothesis_1.hypothesis_1 import Hypothesis1ValidatorCorrect
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score

def create_mid_py_compatible_dataset():
    """åˆ›å»ºä¸mid.pyå…¼å®¹çš„æ•°æ®é›†"""
    print("ğŸ”„ åˆ›å»ºä¸mid.pyå…¼å®¹çš„æ•°æ®é›†...")
    
    validator = Hypothesis1ValidatorCorrect()
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    final_dataset = validator.data_loader.create_final_dataset(
        include_lp_history=True,
        include_reward=True,
        include_discipline=True
    )
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(final_dataset):,} æ¡è®°å½•")
    
    # å…³é”®ä¿®å¤1: å¹´ä»½è¿‡æ»¤ - åªä¿ç•™2011å¹´åŠä»¥åçš„æ•°æ®ï¼ˆä¸mid.pyç¬¬200è¡Œä¸€è‡´ï¼‰
    filtered_dataset = final_dataset[final_dataset['S_YR'] >= 2011].copy()
    print(f"ğŸ“… 2011å¹´åæ•°æ®: {len(filtered_dataset):,} æ¡è®°å½•")
    
    # å…³é”®ä¿®å¤2: åˆ—åç»Ÿä¸€ - å°†RANK_xæ”¹ä¸ºRANKï¼ˆä¸mid.pyä¸€è‡´ï¼‰
    if 'RANK_x' in filtered_dataset.columns:
        filtered_dataset = filtered_dataset.rename(columns={'RANK_x': 'RANK'})
        print("âœ… åˆ—åä¿®å¤: RANK_x -> RANK")
    
    return filtered_dataset

def analyze_with_mid_py_logic(dataframe):
    """ä½¿ç”¨ä¸mid.pyå®Œå…¨ä¸€è‡´çš„é€»è¾‘è¿›è¡Œåˆ†æ"""
    print("ğŸ¯ ä½¿ç”¨mid.pyé€»è¾‘è¿›è¡Œåˆ†æ...")
    
    # Step 1: åˆ›å»ºFISCAL_HALFï¼ˆä¸mid.pyå®Œå…¨ä¸€è‡´ï¼‰
    def get_fiscal_half(row):
        if 4 <= row['S_MO'] <= 9:
            return f"{row['S_YR']}_H1"  # Fiscal first half
        else:
            # Fiscal second half
            return f"{row['S_YR'] - 1}_H2" if row['S_MO'] < 4 else f"{row['S_YR']}_H2"
    
    dataframe['FISCAL_HALF'] = dataframe.apply(get_fiscal_half, axis=1)
    print(f"âœ… FISCAL_HALFåˆ›å»ºå®Œæˆ")
    
    # Step 2: è¿‡æ»¤RANK=10çš„æ•°æ®
    needed_cols = ['LP', 'RANK', 'S_YR', 'S_MO', 'SASHIHIKI', 'AMGR', 'SHOBUN', 'FISCAL_HALF']
    lp_df = dataframe[dataframe['RANK'] == 10][needed_cols].copy()
    print(f"ğŸ“‹ RANK=10æ•°æ®: {len(lp_df):,} æ¡è®°å½•, {lp_df['LP'].nunique():,} ä¸ªLP")
    
    # Step 3 & 4: è®¡ç®—å¹³å‡å€¼ï¼ˆä¸mid.pyå®Œå…¨ä¸€è‡´ï¼‰
    lp_avg_sashihiki = lp_df.groupby(['LP', 'FISCAL_HALF', 'AMGR'])['SASHIHIKI'].mean().reset_index()
    amgr_avg_sashihiki = dataframe.groupby(['AMGR', 'FISCAL_HALF'])['SASHIHIKI'].mean().reset_index()
    amgr_avg_sashihiki.columns = ['AMGR', 'FISCAL_HALF', 'AMGR_AVG_SASHIHIKI']
    
    # Step 5: åˆå¹¶æ•°æ®
    result_df = pd.merge(lp_avg_sashihiki, amgr_avg_sashihiki, on=['AMGR', 'FISCAL_HALF'], how='left')
    
    # Step 6: æ¯”è¾ƒæ“ä½œ
    result_df['Below_AMGR_Avg'] = result_df['SASHIHIKI'] < result_df['AMGR_AVG_SASHIHIKI']
    
    # Step 7: Rolling windowè®¡ç®—ï¼ˆ6ä¸ªè¿ç»­åŠå¹´åº¦ï¼‰
    result_df = result_df.sort_values(['LP', 'FISCAL_HALF'])
    result_df['Rolling_Low'] = result_df.groupby('LP')['Below_AMGR_Avg'].rolling(
        window=6, min_periods=6).sum().reset_index(0, drop=True)
    result_df['Sustained_Low'] = (result_df['Rolling_Low'] >= 6).astype(int)
    
    # Step 8: æ£€æŸ¥ä¸‹ä¸€ä¸ªåŠå¹´åº¦çš„SHOBUN
    shobun_df = lp_df[['LP', 'FISCAL_HALF', 'SHOBUN']].dropna(subset=['SHOBUN'])
    shobun_df['Has_SHOBUN'] = True
    
    # åˆ›å»ºä¸‹ä¸€ä¸ªåŠå¹´åº¦çš„æ˜ å°„
    fiscal_half_order = sorted(result_df['FISCAL_HALF'].unique(), 
                             key=lambda x: (int(x.split('_')[0]), x.split('_')[1]))
    next_half_mapping = dict(zip(fiscal_half_order[:-1], fiscal_half_order[1:]))
    result_df['Next_FISCAL_HALF'] = result_df['FISCAL_HALF'].map(next_half_mapping)
    
    # åˆå¹¶SHOBUNä¿¡æ¯
    result_df = pd.merge(
        result_df,
        shobun_df[['LP', 'FISCAL_HALF', 'Has_SHOBUN']],
        left_on=['LP', 'Next_FISCAL_HALF'],
        right_on=['LP', 'FISCAL_HALF'],
        how='left',
        suffixes=('', '_next')
    )
    
    result_df['SHOBUN_in_Next_Half'] = result_df['Has_SHOBUN'].fillna(False).astype(int)
    
    # æ¸…ç†ä¸´æ—¶åˆ—
    result_df = result_df.drop(['FISCAL_HALF_next', 'Has_SHOBUN', 'Next_FISCAL_HALF', 'Rolling_Low'], axis=1)
    
    print(f"ğŸ“Š åˆ†æå®Œæˆ:")
    print(f"   æ€»è®°å½•æ•°: {len(result_df):,}")
    print(f"   å”¯ä¸€LPæ•°: {result_df['LP'].nunique():,}")
    print(f"   Sustained_Low=1: {(result_df['Sustained_Low'] == 1).sum():,}")
    print(f"   SHOBUN_in_Next_Half=1: {(result_df['SHOBUN_in_Next_Half'] == 1).sum():,}")
    
    return result_df

def create_confusion_matrix_analysis(result_df):
    """åˆ›å»ºæ··æ·†çŸ©é˜µåˆ†æï¼ˆä¸mid.pyå®Œå…¨ä¸€è‡´ï¼‰"""
    print("\nğŸ“ˆ åˆ›å»ºæ··æ·†çŸ©é˜µåˆ†æ...")
    
    # åˆ›å»ºæ··æ·†çŸ©é˜µï¼ˆä¸mid.pyå®Œå…¨ä¸€è‡´ï¼‰
    cm = confusion_matrix(result_df['SHOBUN_in_Next_Half'], result_df['Sustained_Low'])
    f1 = f1_score(result_df['SHOBUN_in_Next_Half'], result_df['Sustained_Low'])
    
    print(f"ğŸ¯ æ··æ·†çŸ©é˜µ:")
    print(cm)
    print(f"ğŸ“Š F1åˆ†æ•°: {f1:.3f}")
    
    # è¯¦ç»†ç»Ÿè®¡
    sustained_low_count = (result_df['Sustained_Low'] == 1).sum()
    shobun_count = (result_df['SHOBUN_in_Next_Half'] == 1).sum()
    both_count = ((result_df['Sustained_Low'] == 1) & (result_df['SHOBUN_in_Next_Half'] == 1)).sum()
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»Ÿè®¡:")
    print(f"   æŒç»­ä½ä¸šç»©æ¡ˆä¾‹æ•°: {sustained_low_count:,}")
    print(f"   ä¸‹æœŸæ‡²æˆ’å‡¦åˆ†æ¡ˆä¾‹æ•°: {shobun_count:,}")
    print(f"   ä¸¤è€…éƒ½æ»¡è¶³: {both_count:,}")
    print(f"   å‡è®¾æ”¯æŒç‡: {both_count/sustained_low_count:.3f}" if sustained_low_count > 0 else "   å‡è®¾æ”¯æŒç‡: 0.000")
    
    return {
        'confusion_matrix': cm,
        'f1_score': f1,
        'sustained_low_count': sustained_low_count,
        'shobun_count': shobun_count,
        'both_count': both_count,
        'total_records': len(result_df)
    }

def compare_with_expected():
    """ä¸å›¾ç‰‡ä¸­çš„é¢„æœŸç»“æœå¯¹æ¯”"""
    print("\nğŸ” ä¸é¢„æœŸç»“æœå¯¹æ¯”:")
    
    expected = {
        'total_cases': 110391,
        'discipline_cases': 1448,
        'f1_score': 0.017
    }
    
    print(f"   æŒ‡æ ‡                | é¢„æœŸç»“æœ    | å½“å‰ç»“æœ    | çŠ¶æ€")
    print(f"   -------------------|-----------|-----------|----------")
    
    return expected

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤Hypothesis 1ä»¥åŒ¹é…mid.pyè®¾ç½®...")
    print("=" * 80)
    
    try:
        # 1. åˆ›å»ºå…¼å®¹æ•°æ®é›†
        dataset = create_mid_py_compatible_dataset()
        
        # 2. ä½¿ç”¨mid.pyé€»è¾‘åˆ†æ
        analysis_results = analyze_with_mid_py_logic(dataset)
        
        # 3. åˆ›å»ºæ··æ·†çŸ©é˜µåˆ†æ
        stats = create_confusion_matrix_analysis(analysis_results)
        
        # 4. ä¸é¢„æœŸç»“æœå¯¹æ¯”
        expected = compare_with_expected()
        
        print(f"\nğŸ“Š æœ€ç»ˆå¯¹æ¯”:")
        print(f"   æ€»æ¡ˆä¾‹æ•°: {stats['total_records']:,} (é¢„æœŸ: {expected['total_cases']:,})")
        print(f"   æ‡²æˆ’å‡¦åˆ†: {stats['shobun_count']:,} (é¢„æœŸ: {expected['discipline_cases']:,})")
        print(f"   F1åˆ†æ•°: {stats['f1_score']:.3f} (é¢„æœŸ: {expected['f1_score']:.3f})")
        
        # è®¡ç®—å·®å¼‚
        case_diff = stats['total_records'] - expected['total_cases']
        discipline_diff = stats['shobun_count'] - expected['discipline_cases']
        f1_diff = stats['f1_score'] - expected['f1_score']
        
        print(f"\nğŸ“ˆ å·®å¼‚åˆ†æ:")
        print(f"   æ€»æ¡ˆä¾‹å·®å¼‚: {case_diff:+,}")
        print(f"   æ‡²æˆ’å‡¦åˆ†å·®å¼‚: {discipline_diff:+,}")
        print(f"   F1åˆ†æ•°å·®å¼‚: {f1_diff:+.3f}")
        
        if abs(case_diff) < 10000 and abs(discipline_diff) < 500:
            print("\nâœ… ç»“æœå·²æ¥è¿‘é¢„æœŸï¼")
        else:
            print("\nâš ï¸  ä»æœ‰å·®å¼‚ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´æ•°æ®è¿‡æ»¤æ¡ä»¶")
        
        print("\n" + "=" * 80)
        print("ğŸ¯ ä¿®å¤å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
